1. create the repo and clone it for base with MIT license and .gitignore file
2. create project flow text file for writing the poitns out
3. define requirements.txt file for dependecies definition
4. create a folder for dataset (https://www.kaggle.com/datasets/nalisha/tesla-ea-deliveries-and-production-data20152025/data)
5. used uv to install requirements.txt libs and used 'uv pip check' to check for compatibility
'uv add <package_name>' to add a library
'uv tool run' or 'uvx <tool>' used to run a tool temporarily

6. got to know best practise is uv sync first to ensure compatibility and then activate venv and then run
7. setup ruff check for linting purposes.As an alternative to uv run, you can also run Ruff by activating the project's virtual environment (source .venv/bin/active on Linux and macOS, or .venv\Scripts\activate on Windows) and running ruff check directly.

8. I will be now focusing on logs and making data_ingestion.py betterly

----so far structured the project so that i can easily add the 
9. after point 8 i will focus on DVC ,testing out on marimo notebook to plan for better steps 
10. worked on ingestion module and rectifying some errors now, tried doing the removeing the try and except to dwell deep into errors, also done ruff checks



** use pytest and pytest.ini for testing along with CI pipelines using github actions
** try to dockerise the model here
** revisit docker notes
-- DVC notes
1. We used dvc get above to show how DVC can turn any Git repo into a "data registry". dvc get can download any file or directory tracked in a DVC repository.
2. 'dvc add data/data.xml' adds data to tracking


https://drive.google.com/drive/u/0/folders/

dvc remote add myremote gdrive://1ejkfvmdcCjhs0-Cib4OdsUqECAxDCl25
